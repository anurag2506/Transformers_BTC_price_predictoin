{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BitcoinDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.FloatTensor(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# BiLSTM Model\n",
    "class BitcoinPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(BitcoinPredictor, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Advanced architecture with multiple BiLSTM layers\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Additional layers for better feature extraction\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,  # *2 for bidirectional\n",
    "            num_heads=4,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers with batch normalization\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        \n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # BiLSTM layers\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_out, _ = self.attention(\n",
    "            lstm_out.permute(1, 0, 2),\n",
    "            lstm_out.permute(1, 0, 2),\n",
    "            lstm_out.permute(1, 0, 2)\n",
    "        )\n",
    "        attention_out = attention_out.permute(1, 0, 2)\n",
    "        \n",
    "        # Take the output from the last time step\n",
    "        out = attention_out[:, -1, :]\n",
    "        \n",
    "        # Fully connected layers with activation and regularization\n",
    "        out = self.fc1(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df, sequence_length=24):\n",
    "    # Use OHLCV data\n",
    "    features = ['open', 'high', 'low', 'close', 'volume']\n",
    "    data = df[features].values\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = MinMaxScaler()\n",
    "    data_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(data_scaled) - sequence_length):\n",
    "        X.append(data_scaled[i:(i + sequence_length)])\n",
    "        y.append(data_scaled[i + sequence_length, 3])  # Predict close price\n",
    "    \n",
    "    # Split data\n",
    "    train_size = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "    \n",
    "    return (np.array(X_train), np.array(X_test), \n",
    "            np.array(y_train), np.array(y_test), \n",
    "            scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                val_loss += criterion(y_pred.squeeze(), y_batch).item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Prediction and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, test_loader, scaler, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_pred = model(X_batch)\n",
    "            predictions.extend(y_pred.cpu().numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "    \n",
    "    # Inverse transform predictions and actuals\n",
    "    predictions = np.array(predictions).reshape(-1, 1)\n",
    "    actuals = np.array(actuals).reshape(-1, 1)\n",
    "    \n",
    "    # Create dummy array for inverse transform\n",
    "    dummy = np.zeros((len(predictions), scaler.scale_.shape[0]))\n",
    "    dummy[:, 3] = predictions.squeeze()  # Close price index\n",
    "    predictions = scaler.inverse_transform(dummy)[:, 3]\n",
    "    \n",
    "    dummy[:, 3] = actuals.squeeze()\n",
    "    actuals = scaler.inverse_transform(dummy)[:, 3]\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(actuals, label='Actual', alpha=0.8)\n",
    "    plt.plot(predictions, label='Predicted', alpha=0.8)\n",
    "    plt.title('Bitcoin Price Prediction')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = np.mean((predictions - actuals) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    \n",
    "    print(f'MSE: {mse:.2f}')\n",
    "    print(f'RMSE: {rmse:.2f}')\n",
    "    print(f'MAE: {mae:.2f}')\n",
    "\n",
    "# Main execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load your data\n",
    "    df = pd.read_csv('data_hr.csv')  # Adjust path as needed\n",
    "    \n",
    "    # Model parameters\n",
    "    sequence_length = 24  # 24 hours of data\n",
    "    input_size = 5  # OHLCV features\n",
    "    hidden_size = 128\n",
    "    num_layers = 3\n",
    "    dropout = 0.3\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test, scaler = prepare_data(df, sequence_length)\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = BitcoinDataset(X_train, y_train)\n",
    "    test_dataset = BitcoinDataset(X_test, y_test)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Initialize model and training components\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BitcoinPredictor(input_size, hidden_size, num_layers, dropout).to(device)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    train_losses, val_losses = train_model(\n",
    "        model, train_loader, test_loader, criterion, optimizer, \n",
    "        epochs=40, device=device\n",
    "    )\n",
    "    \n",
    "    # Visualize predictions\n",
    "    visualize_predictions(model, test_loader, scaler, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anurag2506/miniconda3/envs/mentoroid/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Train Loss: 0.1972, Val Loss: 0.0024\n",
      "Epoch [2/40], Train Loss: 0.0433, Val Loss: 0.0029\n",
      "Epoch [3/40], Train Loss: 0.0257, Val Loss: 0.0016\n",
      "Epoch [4/40], Train Loss: 0.0185, Val Loss: 0.0005\n",
      "Epoch [5/40], Train Loss: 0.0146, Val Loss: 0.0015\n",
      "Epoch [6/40], Train Loss: 0.0125, Val Loss: 0.0003\n",
      "Epoch [7/40], Train Loss: 0.0107, Val Loss: 0.0001\n",
      "Epoch [8/40], Train Loss: 0.0096, Val Loss: 0.0001\n",
      "Epoch [9/40], Train Loss: 0.0090, Val Loss: 0.0001\n",
      "Epoch [10/40], Train Loss: 0.0078, Val Loss: 0.0005\n",
      "Epoch [11/40], Train Loss: 0.0074, Val Loss: 0.0002\n",
      "Epoch [12/40], Train Loss: 0.0065, Val Loss: 0.0002\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mentoroid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
